{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Face Recognization with VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's re-sizing the image to 64 x 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 2268s 39us/step\n",
      "0 InputLayer False\n",
      "1 Conv2D False\n",
      "2 Conv2D False\n",
      "3 MaxPooling2D False\n",
      "4 Conv2D False\n",
      "5 Conv2D False\n",
      "6 MaxPooling2D False\n",
      "7 Conv2D False\n",
      "8 Conv2D False\n",
      "9 Conv2D False\n",
      "10 MaxPooling2D False\n",
      "11 Conv2D False\n",
      "12 Conv2D False\n",
      "13 Conv2D False\n",
      "14 MaxPooling2D False\n",
      "15 Conv2D False\n",
      "16 Conv2D False\n",
      "17 Conv2D False\n",
      "18 MaxPooling2D False\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "# Setting the input size now to 64 x 64 pixel \n",
    "img_rows = 64\n",
    "img_cols = 64 \n",
    "\n",
    "# Re-loads the VGG16 model without the top or FC layers\n",
    "vgg16 = VGG16(weights = 'imagenet', \n",
    "                 include_top = False, \n",
    "                 input_shape = (img_rows, img_cols, 3))\n",
    "\n",
    "# Layers are set to trainable as True by default\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(vgg16.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTopModel(bottom_model, num_classes, D=256):\n",
    "    \"\"\"creates the top or head of the model that will be \n",
    "    placed ontop of the bottom layers\"\"\"\n",
    "    top_model = bottom_model.output\n",
    "    top_model = Flatten(name = \"flatten\")(top_model)\n",
    "    top_model = Dense(D, activation = \"relu\")(top_model)\n",
    "    top_model = Dropout(0.3)(top_model)\n",
    "    top_model = Dense(num_classes, activation = \"sigmoid\")(top_model)\n",
    "    return top_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create our new model using an image size of 64 x 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n",
      "Found 200 images belonging to 2 classes.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 15,239,746\n",
      "Trainable params: 525,058\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir = 'dataset/train/'\n",
    "validation_data_dir = 'dataset/validation/'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 16\n",
    "val_batchsize = 10\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical')\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "# Re-loads the VGG16 model without the top or FC layers\n",
    "vgg16 = VGG16(weights = 'imagenet', \n",
    "                 include_top = False, \n",
    "                 input_shape = (img_rows, img_cols, 3))\n",
    "\n",
    "# Freeze layers\n",
    "for layer in vgg16.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Number of classes in the dataset                                                \n",
    "num_classes = 2\n",
    "\n",
    "FC_Head = addTopModel(vgg16, num_classes)\n",
    "\n",
    "model = Model(inputs=vgg16.input, outputs=FC_Head)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using 64 x 64 image size is MUCH faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "25/25 [==============================] - 4s 164ms/step - loss: 0.4434 - accuracy: 0.8075 - val_loss: 0.1151 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11515, saving model to Face_Recog.h5\n",
      "Epoch 2/25\n",
      "25/25 [==============================] - 4s 161ms/step - loss: 0.2175 - accuracy: 0.9375 - val_loss: 0.0341 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11515 to 0.03406, saving model to Face_Recog.h5\n",
      "Epoch 3/25\n",
      "25/25 [==============================] - 3s 133ms/step - loss: 0.1220 - accuracy: 0.9825 - val_loss: 0.0176 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03406 to 0.01760, saving model to Face_Recog.h5\n",
      "Epoch 4/25\n",
      "25/25 [==============================] - 4s 144ms/step - loss: 0.0812 - accuracy: 0.9825 - val_loss: 0.0070 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.01760 to 0.00704, saving model to Face_Recog.h5\n",
      "Epoch 5/25\n",
      "25/25 [==============================] - 4s 145ms/step - loss: 0.0499 - accuracy: 0.9950 - val_loss: 0.0080 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00704\n",
      "Epoch 6/25\n",
      "25/25 [==============================] - 4s 154ms/step - loss: 0.0445 - accuracy: 0.9875 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00704 to 0.00118, saving model to Face_Recog.h5\n",
      "Epoch 7/25\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 0.0256 - accuracy: 0.9950 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00118\n",
      "Epoch 8/25\n",
      "25/25 [==============================] - 5s 184ms/step - loss: 0.0219 - accuracy: 0.9950 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00118\n",
      "Epoch 9/25\n",
      "25/25 [==============================] - 5s 185ms/step - loss: 0.0170 - accuracy: 0.9975 - val_loss: 6.3100e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00118 to 0.00006, saving model to Face_Recog.h5\n",
      "Epoch 10/25\n",
      "25/25 [==============================] - 4s 168ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 4.3580e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00006\n",
      "Epoch 11/25\n",
      "25/25 [==============================] - 4s 167ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 3.4865e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00006\n",
      "Epoch 12/25\n",
      "25/25 [==============================] - 4s 166ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 1.0204e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00006 to 0.00001, saving model to Face_Recog.h5\n",
      "Epoch 13/25\n",
      "25/25 [==============================] - 4s 166ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 4.3131e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.00001\n",
      "Epoch 14/25\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 1.6147e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00001\n",
      "Epoch 15/25\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 1.2185e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "Epoch 16/25\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 3.2455e-06 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00001 to 0.00000, saving model to Face_Recog.h5\n",
      "Epoch 17/25\n",
      "25/25 [==============================] - 5s 184ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 4.5528e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00000\n",
      "Epoch 18/25\n",
      "25/25 [==============================] - 5s 180ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 4.3474e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 3.999999898951501e-06.\n",
      "Epoch 19/25\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 0.0051 - accuracy: 0.9975 - val_loss: 6.7353e-07 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to Face_Recog.h5\n",
      "Epoch 20/25\n",
      "25/25 [==============================] - 4s 173ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 7.3901e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00000\n",
      "Epoch 21/25\n",
      "25/25 [==============================] - 4s 173ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 3.8939e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 7.999999979801942e-07.\n",
      "Epoch 22/25\n",
      "25/25 [==============================] - 4s 179ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 6.8247e-07 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00000\n",
      "Epoch 23/25\n",
      "25/25 [==============================] - 5s 184ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 9.7782e-06 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00000\n",
      "Epoch 24/25\n",
      "25/25 [==============================] - 5s 182ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 2.3196e-05 - val_accuracy: 1.0000\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.600000018697756e-07.\n",
      "Epoch 00024: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "                   \n",
    "checkpoint = ModelCheckpoint(\"Face_Recog.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 5,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.2,\n",
    "                              patience = 3,\n",
    "                              verbose = 1,\n",
    "                              min_delta = 0.00001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "# Note we use a very small learning rate \n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.0001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 800                                      \n",
    "nb_validation_samples = 200\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)\n",
    "\n",
    "model.save(\"Face_Recog.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_photo = image.load_img(\"predictabhi.jpg\", target_size=(64,64,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_photo = image.img_to_array(my_photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_photo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_photo = np.expand_dims(my_photo, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 64, 64, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_photo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_photo = preprocess_input(my_photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(my_photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abhishek\n"
     ]
    }
   ],
   "source": [
    "if result[0][0] == 1.0:\n",
    "    print(\"Abhishek\")\n",
    "if result[0][1] == 1.0:\n",
    "    print(\"Mom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
